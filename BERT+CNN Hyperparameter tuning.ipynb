{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e052585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tokenization\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47862f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   polarity rating                                        review_text  \\\n",
      "0  negative    2.0  The Easy Language 16 is only useful if you're ...   \n",
      "1  negative    1.0  My boss asked me to get this, so that he check...   \n",
      "2  negative    1.0  I found this title messing around adding thing...   \n",
      "3  negative    1.0  They have the hebrew backwards!! You are suppo...   \n",
      "4  negative    2.0  I just bought and installed this CD with the h...   \n",
      "\n",
      "   polarity_encoded  \n",
      "0                 0  \n",
      "1                 0  \n",
      "2                 0  \n",
      "3                 0  \n",
      "4                 0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>polarity_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The Easy Language 16 is only useful if you're ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My boss asked me to get this, so that he check...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I found this title messing around adding thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>They have the hebrew backwards!! You are suppo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I just bought and installed this CD with the h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>positive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>It was some years ago when I bought a previous...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>positive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The hard-copy of Britannica has been the stand...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>positive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I trust the information I find from Britannica...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>positive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>If you want a simple to use program to manage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>positive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have used Quicken for over 12 years.  Whenev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1915 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      polarity rating                                        review_text  \\\n",
       "0     negative    2.0  The Easy Language 16 is only useful if you're ...   \n",
       "1     negative    1.0  My boss asked me to get this, so that he check...   \n",
       "2     negative    1.0  I found this title messing around adding thing...   \n",
       "3     negative    1.0  They have the hebrew backwards!! You are suppo...   \n",
       "4     negative    2.0  I just bought and installed this CD with the h...   \n",
       "...        ...    ...                                                ...   \n",
       "1910  positive    5.0  It was some years ago when I bought a previous...   \n",
       "1911  positive    5.0  The hard-copy of Britannica has been the stand...   \n",
       "1912  positive    4.0  I trust the information I find from Britannica...   \n",
       "1913  positive    4.0  If you want a simple to use program to manage ...   \n",
       "1914  positive    5.0  I have used Quicken for over 12 years.  Whenev...   \n",
       "\n",
       "      polarity_encoded  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "...                ...  \n",
       "1910                 1  \n",
       "1911                 1  \n",
       "1912                 1  \n",
       "1913                 1  \n",
       "1914                 1  \n",
       "\n",
       "[1915 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert import bert_tokenization\n",
    "#from textblob.en import polarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#from textblob import TextBlob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xml.etree.ElementTree as ET\n",
    "#import xmltodict\n",
    "\n",
    "# # Reading the dataset into a dataframe\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "\n",
    "tree = ET.parse('../unprocessed.tar/sorted_data/sorted_data/software/negative.review')\n",
    "root = tree.getroot()\n",
    "\n",
    "data = []\n",
    "for review in root:\n",
    "    review_dict = {}\n",
    "    for elem in review:\n",
    "        if elem.tag in ['review_text', 'rating']:\n",
    "            review_dict[elem.tag] = elem.text.strip()\n",
    "        review_dict[\"polarity\"] = \"negative\"\n",
    "    data.append(review_dict)\n",
    "\n",
    "tree = ET.parse('../unprocessed.tar/sorted_data/sorted_data/software/positive.review')\n",
    "root = tree.getroot()\n",
    "for review in root:\n",
    "    review_dict = {}\n",
    "    for elem in review:\n",
    "        if elem.tag in ['review_text', 'rating']:\n",
    "            review_dict[elem.tag] = elem.text.strip()\n",
    "        review_dict[\"polarity\"] = \"positive\"\n",
    "    data.append(review_dict)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['polarity_encoded'] = df['polarity'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "print(df.head())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b1ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer  used:  adam  Activation Function used :  relu\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "{'lr': 2e-05, 'num_filters': 32, 'optimizer': 'adam', 'activation': 'relu'}\n",
      "Optimizer  used:  adam  Activation Function used :  sigmoid\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, GlobalMaxPooling1D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import re\n",
    "\n",
    "# Preprocess the text data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 128\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = tokens[:max_len-2]\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    padding = [0] * (max_len - len(input_ids))\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    return input_ids, input_mask\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "x_train = [preprocess(text) for text in train_df['review_text']]\n",
    "y_train = train_df['polarity_encoded']\n",
    "x_val = [preprocess(text) for text in val_df['review_text']]\n",
    "y_val = val_df['polarity_encoded']\n",
    "x_test = [preprocess(text) for text in test_df['review_text']]\n",
    "y_test = test_df['polarity_encoded']\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#learning_rates = [1e-4, 1e-5, 1e-6]\n",
    "#num_filters = [16, 32, 64]\n",
    "optimizers = ['adam', 'SGD']\n",
    "activations = ['relu', 'sigmoid']\n",
    "\n",
    "# initialize the best score and best hyperparameters\n",
    "best_score = 0\n",
    "best_hyperparams = {}\n",
    "\n",
    "# loop through all combinations of hyperparameters\n",
    "for opt in optimizers:\n",
    "    for act in activations:\n",
    "        # build the model with the current hyperparameters\n",
    "        print(\"Optimizer  used: \", opt, \" Activation Function used : \", act)\n",
    "        input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "        input_mask = Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n",
    "        embedding = bert_model(input_ids, attention_mask=input_mask)[0]\n",
    "        conv1 = Conv1D(filters=32, kernel_size=3, padding='same', activation=act)(embedding)\n",
    "        conv2 = Conv1D(filters=32, kernel_size=4, padding='same', activation=act)(embedding)\n",
    "        conv3 = Conv1D(filters=32, kernel_size=5, padding='same', activation=act)(embedding)\n",
    "        concat = concatenate([conv1, conv2, conv3])\n",
    "        pooling = GlobalMaxPooling1D()(concat)\n",
    "        dropout = Dropout(0.2)(pooling)\n",
    "        output = Dense(1, activation='sigmoid')(dropout)\n",
    "        model = Model(inputs=[input_ids, input_mask], outputs=output)\n",
    "\n",
    "        # compile the model with the current hyperparameters\n",
    "\n",
    "        if opt == 'adam':\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "        elif opt == 'SGD':\n",
    "            optimizer = keras.optimizers.SGD(learning_rate=2e-5)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # train the model and evaluate on the validation set\n",
    "        history = model.fit(x=[np.array(x_train)[:,0], np.array(x_train)[:,1]], y=y_train,\n",
    "                            validation_data=([np.array(x_val)[:,0], np.array(x_val)[:,1]], y_val),\n",
    "                            epochs=3, batch_size=32, verbose=0)\n",
    "        score = history.history['val_accuracy'][-1]\n",
    "\n",
    "        # update the best score and best hyperparameters if the current model is better\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_hyperparams = {'lr': 2e-5, 'num_filters': 32, 'optimizer': opt, 'activation': act}\n",
    "            print(best_hyperparams)\n",
    "\n",
    "# train the model with the best hyperparameters on the full training set\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n",
    "embedding = bert_model(input_ids, attention_mask=input_mask)[0]\n",
    "conv1 = Conv1D(filters=best_hyperparams['num_filters'], kernel_size=3, padding='same', activation=best_hyperparams['activation'])(embedding)\n",
    "conv2 = Conv1D(filters=best_hyperparams['num_filters'], kernel_size=4, padding='same', activation=best_hyperparams['activation'])(embedding)\n",
    "conv3 = Conv1D(filters=best_hyperparams['num_filters'], kernel_size=5, padding='same', activation=best_hyperparams['activation'])(embedding)\n",
    "concat = concatenate([conv1, conv2, conv3])\n",
    "pooling = GlobalMaxPooling1D()(concat)\n",
    "dropout = Dropout(0.2)(pooling)\n",
    "output = Dense(1, activation='sigmoid')(dropout)\n",
    "model = Model(inputs=[input_ids, input_mask], outputs=output)\n",
    "\n",
    "optimizer_name = best_hyperparams['optimizer']\n",
    "if optimizer_name == 'adam':\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr, epsilon=1e-08)\n",
    "elif optimizer_name == 'SGD':\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# # Build the CNN model on top of the BERT model\n",
    "# input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "# input_mask = Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n",
    "# embedding = bert_model(input_ids, attention_mask=input_mask)[0]\n",
    "# conv1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(embedding)\n",
    "# conv2 = Conv1D(filters=32, kernel_size=4, padding='same', activation='relu')(embedding)\n",
    "# conv3 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(embedding)\n",
    "# concat = concatenate([conv1, conv2, conv3])\n",
    "# pooling = GlobalMaxPooling1D()(concat)\n",
    "# dropout = Dropout(0.2)(pooling)\n",
    "# output = Dense(1, activation='sigmoid')(dropout)\n",
    "# model = Model(inputs=[input_ids, input_mask], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "history = model.fit(x=[np.array(x_train)[:,0], np.array(x_train)[:,1]], y=y_train,\n",
    "                    validation_data=([np.array(x_val)[:,0], np.array(x_val)[:,1]], y_val),\n",
    "                    epochs=3, batch_size=32)\n",
    "\n",
    "# # Evaluate the model\n",
    "# model.evaluate([np.array(x_test)[:,0], np.array(x_test)[:,1]], y_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict([np.array(x_test)[:,0], np.array(x_test)[:,1]])\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ae7729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 76s 6s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       180\n",
      "           1       0.90      0.91      0.90       203\n",
      "\n",
      "    accuracy                           0.90       383\n",
      "   macro avg       0.90      0.90      0.90       383\n",
      "weighted avg       0.90      0.90      0.90       383\n",
      "\n",
      "[[159  21]\n",
      " [ 18 185]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict([np.array(x_test)[:,0], np.array(x_test)[:,1]])\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af5cc833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 357ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['positive']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the sentiment of new review_text data\n",
    "new_review_text = 'This book is amazing!'\n",
    "input_ids, input_mask = preprocess(new_review_text)\n",
    "test_input_pred = model.predict([np.array([input_ids]), np.array([input_mask])])\n",
    "test_input_pred = [\"positive\" if p >= 0.5 else \"negative\" for p in test_input_pred]\n",
    "test_input_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f2d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
